return {
  'huggingface/llm.nvim',
  opts = {
    backend = "ollama",
    model = "qwen2.5-coder:1.5b",
    url = "http://localhost:11434", -- llm-ls uses "/api/generate"
    accept_keymap = "<C-l>",
    -- cf https://github.com/ollama/ollama/blob/main/docs/api.md#parameters
    -- request_body = {
    --   -- Modelfile options for the model you use
    --   options = {
    --     temperature = 0.2,
    --     top_p = 0.95,
    --   }
    -- }   -- cf Setup
  }
}
